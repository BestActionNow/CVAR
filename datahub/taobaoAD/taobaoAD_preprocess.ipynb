{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e009205b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff8e2461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read raw data\n",
    "raw_sample = pd.read_csv('./raw_sample.csv', engine='c')\n",
    "ad_feature = pd.read_csv('./ad_feature.csv', engine='c')\n",
    "user_profile = pd.read_csv('./user_profile.csv', engine = 'c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab475434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join samples\n",
    "sample = raw_sample.join(ad_feature.set_index('adgroup_id'), on='adgroup_id')\n",
    "sample = sample.join(user_profile.set_index('userid'), on='user')\n",
    "sample = sample.rename(columns = {\"adgroup_id\": \"item_id\", \"user\": \"user_id\"})\n",
    "sample = sample.drop('nonclk', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d0fc1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id\n",
      "item_id\n",
      "pid\n",
      "cate_id\n",
      "campaign_id\n",
      "customer\n",
      "brand\n",
      "cms_segid\n",
      "cms_group_id\n",
      "final_gender_code\n",
      "age_level\n",
      "pvalue_level\n",
      "shopping_level\n",
      "occupation\n",
      "new_user_class_level \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('user_id', 1141730, 'spr'),\n",
       " ('item_id', 846812, 'spr'),\n",
       " ('pid', 3, 'spr'),\n",
       " ('cate_id', 6770, 'spr'),\n",
       " ('campaign_id', 423437, 'spr'),\n",
       " ('customer', 255876, 'spr'),\n",
       " ('brand', 99816, 'spr'),\n",
       " ('cms_segid', 99, 'spr'),\n",
       " ('cms_group_id', 15, 'spr'),\n",
       " ('final_gender_code', 4, 'spr'),\n",
       " ('age_level', 9, 'spr'),\n",
       " ('pvalue_level', 5, 'spr'),\n",
       " ('shopping_level', 5, 'spr'),\n",
       " ('occupation', 4, 'spr'),\n",
       " ('new_user_class_level ', 6, 'spr'),\n",
       " ('time_stamp', -1, 'ctn'),\n",
       " ('price', -1, 'ctn'),\n",
       " ('count', -1, 'ctn'),\n",
       " ('clk', 2, 'label')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define unique function\n",
    "def _unique(sample, fname):\n",
    "    tmp_df = pd.DataFrame()\n",
    "    tmp_df[fname] = sample[fname].unique()\n",
    "    num = len(tmp_df)\n",
    "    tmp_df['tmp_feature'] = range(num)\n",
    "    sample = sample.join(tmp_df.set_index(fname), on=fname)\n",
    "    sample.drop(fname, axis=1, inplace=True)\n",
    "    sample = sample.rename(columns = {\"tmp_feature\": fname})\n",
    "    return num, sample\n",
    "\n",
    "# preprocess fequency\n",
    "item2count = sample.groupby(['item_id']).size().reset_index(name='count').sort_values(by='count')\n",
    "sample = sample.join(item2count.set_index('item_id'), on='item_id')\n",
    "\n",
    "# preprocess and generate descripition\n",
    "spr_features = ['user_id', 'item_id', 'pid', 'cate_id', 'campaign_id', 'customer', 'brand', 'cms_segid', \\\n",
    "       'cms_group_id', 'final_gender_code', 'age_level', 'pvalue_level', \\\n",
    "       'shopping_level', 'occupation', 'new_user_class_level ']\n",
    "ctn_features = ['time_stamp', 'price', 'count']\n",
    "label = ['clk']\n",
    "description = []\n",
    "for spr_f in spr_features:\n",
    "    print(spr_f) \n",
    "    num, sample = _unique(sample, spr_f)\n",
    "    num += 1\n",
    "    sample[spr_f] = sample[spr_f].fillna(num - 1)\n",
    "    sample[spr_f] = sample[spr_f].astype('int')\n",
    "    description.append((spr_f, num, 'spr'))\n",
    "for ctn_f in ctn_features:\n",
    "    sample[ctn_f] = sample[ctn_f].fillna(0.0)\n",
    "    min_v = np.min(sample[ctn_f])\n",
    "    max_v = np.max(sample[ctn_f])\n",
    "    sample[ctn_f] = sample[ctn_f].map(lambda x: (x - min_v)/(max_v - min_v))\n",
    "    description.append((ctn_f, -1, 'ctn'))\n",
    "for l in label:\n",
    "    description.append((l, 2, 'label'))\n",
    "description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "554f85b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pid0 = sample[sample['pid']==0][['item_id', 'user_id']]\n",
    "x_pid1 = sample[sample['pid']==1][['item_id', 'user_id']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ca99373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13767\n"
     ]
    }
   ],
   "source": [
    "print(len(set(x_pid0['user_id']).intersection(set(x_pid1['user_id']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db989881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_base size: 3592047\n",
      "train_warm_a size: 270500\n",
      "train_warm_b size: 270500\n",
      "train_warm_c size: 270500\n",
      "test size: 109712\n",
      "description size: 19\n"
     ]
    }
   ],
   "source": [
    "# split dataset\n",
    "N, K = 2000, 500\n",
    "item2count = sample.groupby(['item_id']).size().reset_index(name='count').sort_values(by='count')\n",
    "item_ids = list(item2count['item_id'])\n",
    "counts = np.array(item2count['count'])\n",
    "\n",
    "item_ids, counts = np.asarray(item_ids), np.asarray(counts)\n",
    "hot_item_ids = item_ids[counts > N]\n",
    "cold_item_ids = item_ids[np.logical_and(counts <= N, counts >= 3 * K)]\n",
    "item_group = sample.groupby('item_id')\n",
    "train_base = pd.DataFrame()\n",
    "for item_id in hot_item_ids:\n",
    "    df_hot = item_group.get_group(item_id).sort_values(by='time_stamp')\n",
    "    train_base = train_base.append(df_hot, ignore_index=True)\n",
    "train_warm_a, train_warm_b, train_warm_c, test = pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "for item_id in cold_item_ids:\n",
    "    df_cold = item_group.get_group(item_id).sort_values(by='time_stamp')\n",
    "    train_warm_a = train_warm_a.append(df_cold[: K], ignore_index=True)\n",
    "    train_warm_b = train_warm_b.append(df_cold[K: 2*K], ignore_index=True)\n",
    "    train_warm_c = train_warm_c.append(df_cold[2*K: 3*K], ignore_index=True)\n",
    "    test = test.append(df_cold[3*K:], ignore_index=True)\n",
    "save_dic = {\n",
    "    'train_base': train_base.sort_values('time_stamp'),\n",
    "    'train_warm_a': train_warm_a.sort_values('time_stamp'),\n",
    "    'train_warm_b': train_warm_b.sort_values('time_stamp'),\n",
    "    'train_warm_c': train_warm_c.sort_values('time_stamp'),\n",
    "    'test': test.sort_values('time_stamp'),\n",
    "    'description': description\n",
    "}\n",
    "for name, df in save_dic.items():\n",
    "    print(\"{} size: {}\".format(name, len(df)))\n",
    "with open('./emb_warm_split_preprocess_taobao-ad.pkl', 'bw+') as f:\n",
    "    pickle.dump(save_dic, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72135335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the train sef for Meta-Embedding method\n",
    "with open('./emb_warm_split_preprocess_taobao-ad.pkl', 'rb+') as f:\n",
    "    data = pickle.load(f)\n",
    "    df_base = data['train_base']\n",
    "item2group = df_base.groupby('item_id')\n",
    "train_a, train_b, train_c, train_d = pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "for item_id, df_group in item2group:\n",
    "    l, e = df_group.shape[0], df_group.shape[0] // 4     \n",
    "    train_a = train_a.append(df_group.iloc[0: e,], ignore_index=True)\n",
    "    train_b = train_b.append(df_group.iloc[e: 2 * e, ], ignore_index=True)\n",
    "    train_c = train_c.append(df_group.iloc[2 * e: 3 * e, ], ignore_index=True)\n",
    "    train_d = train_d.append(df_group.iloc[3 * e: 4 * e, ], ignore_index=True)\n",
    "shuffle_idx = np.random.permutation(train_a.shape[0])\n",
    "train_a = train_a.iloc[shuffle_idx]\n",
    "train_b = train_b.iloc[shuffle_idx]\n",
    "train_c = train_c.iloc[shuffle_idx]\n",
    "train_d = train_d.iloc[shuffle_idx]\n",
    "data[\"metaE_a\"] = train_a\n",
    "data[\"metaE_b\"] = train_b\n",
    "data[\"metaE_c\"] = train_c\n",
    "data[\"metaE_d\"] = train_d\n",
    "with open('./taobaoAD_data.pkl', 'wb+') as f:\n",
    "    pickle.dump(data, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_test_python",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
